{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The 'app.py' is the main Streamlit app that allows users to analyze and explore text data. Here's an explanation of the functions and methods used in the code:\n",
    "- **text_analyzer(my_text):** This function takes a text as input and performs analysis on it using spaCy. It extracts various attributes for each token in the text, such as token shape, part-of-speech tag, lemma, and whether it is alphabetic or a stopword. The function returns the analysis results as a pandas DataFrame.\n",
    "- **get_entities(my_text):** This function takes a text as input and uses spaCy to extract named entities from the text. It returns a list of tuples, where each tuple contains the text of the entity and its label.\n",
    "- **render_entitis(rawtext):** This function takes raw text as input and uses spaCy's displacy module to visualize the named entities in the text. It returns an HTML string containing the rendered visualization.\n",
    "- **plot_wordcloud(text):** This function generates a word cloud visualization from the given text using the WordCloud module from the wordcloud library. It removes stopwords and generates a word cloud based on the word frequencies in the text.\n",
    "- **extract_date(texts) and extract_date2(texts):** These functions extract dates from the given text using the datefinder library. The first function returns a single date, while the second function returns a list of dates.\n",
    "- **analyze_sentiment(letters) and analyze_sentiment1(documents):** These functions analyze the sentiment of each letter or document using the TextBlob library. They calculate the sentiment polarity (positive, negative, or neutral) and return a list of tuples containing the letter/document, date, and sentiment.\n",
    "- **analyze_temporal_spatial(letters):** This function associates each letter with a specific location (latitude and longitude) and returns a list of tuples containing the letter, date, latitude, and longitude. The location data is hardcoded in the lat and long lists.\n",
    "- **plot_pos_histogram(text):** This function generates a histogram plot showing the distribution of part-of-speech tags in the given text. It uses the Counter class from the collections module and seaborn for visualization.\n",
    "- **plot_word_freq(df, num_of_most_common):** This function generates a bar plot showing the most common words and their frequencies in the given DataFrame. It uses the barplot function from the seaborn library.\n",
    "- **create_map():** This function creates an interactive map using the folium library. It adds circle markers on the map based on latitude, longitude, ratings, and the number of letters received.\n",
    "- **word_stats(text):** This function calculates the number of tokens, sentences, words, and stopwords in the given text using spaCy.\n",
    "- **preprocess(text):** This function preprocesses the text by tokenizing it, removing stopwords, and lemmatizing the words. It returns a list of preprocessed words.\n",
    "- **create_dictionary_and_corpus(texts):** This function preprocesses a list of texts, creates a dictionary, and converts the texts into bag-of-words representation (corpus) using the gensim library.\n",
    "- **get_lda_topics(texts, num_topics, passes):** This function performs Latent Dirichlet Allocation (LDA) topic modeling on the given texts. It creates a dictionary and corpus using create_dictionary_and_corpus(), and then trains an LDA model using the ldamodel\n",
    "\n",
    "### 2. The 'rdflib_vespasiano.ipynb' notebook performs a sequence of data extraction, transformation, and loading (ETL) operations on the RDF dataset and it consists of three parts as follows.\n",
    "**The goal of the first part of the script is to load and parse the RDF dataset, extract relevant information from it, and store the extracted data for further analysis. This involves RDF data manipulation, data validation, and the creation of a CSV file and a pandas DataFrame to store the processed data.\n",
    "The steps can be summarized as follows:**\n",
    "1. **Importing Libraries:** Essential libraries, such as rdflib, are imported for RDF manipulation.\n",
    "2. **Loading RDF data:** RDF data is loaded from a file stored in Google Drive and a graph is created for manipulation.\n",
    "3. **Exploring the Data:** Initial data exploration is conducted to understand the dataset's structure and content.\n",
    "4. **Data Extraction:** Information about the content of various letters is extracted and stored in a list. This includes identifying the content of a specific letter, creating a list of letter URIs, and creating a list of letter contents.\n",
    "5. **Data Validation:** A check is performed to ensure that the number of letter URIs and their contents match.\n",
    "6. **DataFrame Creation and Saving:** A DataFrame is created to store the extracted letter contents and their URIs, and this DataFrame is then saved to a CSV file.\n",
    "7. **Additional Data Extraction:** Additional information about another specific letter is extracted and stored as a pandas Series.\n",
    "\n",
    "**The second part of the script focuses on extracting temporal information, i.e., dates from the dataset. It uses regular expressions to find and sort dates, attaches this date information to the previously extracted letter content, and saves this combined data for future use.**\n",
    "The steps can be summarized as follows:\n",
    "1. **Listing Letters:** A list of letters (excluding a problematic one) is created.\n",
    "2. **Date Collection:** For each letter, the date of writing is identified from the RDF dataset and stored in a DataFrame.\n",
    "3. **Sorting Dates:** Regular expressions are used to sort the dates based on their formatting. Any discrepancies in the dates are addressed.\n",
    "4. **Date Update:** The sorted dates replace the original dates in the DataFrame.\n",
    "5. **Data Consolidation:** The date information is added to the table of letters and contents, combining all relevant information in one place.\n",
    "6. **Data Export:** The consolidated data, consisting of letters, their content, and the dates, is saved to a new CSV file named 'letters-with-dates.csv'.\n",
    "\n",
    "**The third part focuess on enriching the DataFrame extended_fianl with additional information drawn from the RDF graph. This part is focused on refining the uri_from column and adding a few new columns to the DataFrame.**\n",
    "Here is a summary of the objectives of this part:\n",
    "1. **Refinement of Place Information:** The uri_from column, which stores the place information, is further refined. It was initially filled based on the uri_maintext column, but in this part, another approach is also used to update the uri_from values. This second approach uses the 'p-sender-letter' URIs instead of the main text URIs to find and update the corresponding place information.\n",
    "2. **Addition of New Columns:** The DataFrame is enriched with additional columns that represent new information:\n",
    "- uri_expr: The URI of the expressions associated with each letter.\n",
    "- sender-att: The URI of the sender attributions associated with each expression.\n",
    "- p-sender-letter: The URI of the entity generated by each sender attribution.\n",
    "3. **Data Export:** The final DataFrame, now with enriched information, is exported to a CSV file named 'extended-data.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
